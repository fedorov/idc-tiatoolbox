{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tissue Region Segmentation of IDC Slides with TIAToolbox\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/fedorov/idc-tiatoolbox/blob/main/notebooks/05_semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "While [Notebook 04](04_patch_classification.ipynb) classified patches into tissue types, **semantic segmentation** provides pixel-level tissue classification, creating detailed maps of tissue regions.\n",
    "\n",
    "In this notebook, we use TIAToolbox's `SemanticSegmentor` with the **FCN-ResNet50-UNet-BCSS** model to segment breast cancer tissue from an IDC slide into regions:\n",
    "\n",
    "- Tumor\n",
    "- Stroma\n",
    "- Inflammatory infiltrate\n",
    "- Necrosis\n",
    "- Other\n",
    "\n",
    "The model was trained on the [BCSS (Breast Cancer Semantic Segmentation)](https://bcsegmentation.grand-challenge.org/) dataset.\n",
    "\n",
    "**GPU recommended** for faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Installation\n\nRun the cell below to install dependencies. **On Colab, the runtime will automatically restart** after installation to pick up the updated numpy version. After the restart, continue from the imports cell below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%pip install tiatoolbox idc-index openslide-bin \"numcodecs<0.16\"\n\n# Restart runtime to pick up updated numpy (required on Colab)\nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nfrom idc_index import IDCClient\nfrom tiatoolbox.wsicore.wsireader import WSIReader\nfrom tiatoolbox.models.engine.semantic_segmentor import (\n    SemanticSegmentor,\n    IOSegmentorConfig,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    try:\n        torch.zeros(1, device=\"cuda\")\n    except RuntimeError:\n        print(\"CUDA available but not functional â€” falling back to CPU\")\n        device = \"cpu\"\nprint(f\"Using device: {device}\")\nif device == \"cpu\":\n    print(\"Note: GPU is recommended. In Colab: Runtime > Change runtime type > T4 GPU\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Reproducibility Information\n\nCapture execution timestamp and environment details for reproducibility.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import datetime, os, platform, subprocess, sys\nfrom importlib.metadata import version, PackageNotFoundError\n\nprint(f\"Executed: {datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\nprint(f\"Platform: {platform.platform()}\")\nprint(f\"Python:   {sys.version}\")\n\n# Detect Colab environment and runtime version\ntry:\n    import google.colab  # noqa: F401\n    colab_env = \"Google Colab\"\n    # Try to get the Colab runtime version (e.g., \"2026.01\")\n    colab_release = os.environ.get(\"COLAB_RELEASE_TAG\")\n    if colab_release:\n        colab_env += f\" (runtime {colab_release})\"\n    else:\n        # Fall back to google-colab package version as a proxy\n        try:\n            colab_env += f\" (google-colab {version('google-colab')})\"\n        except PackageNotFoundError:\n            pass\n    print(f\"Runtime:  {colab_env}\")\nexcept ImportError:\n    print(\"Runtime:  Local\")\n\nprint(\"\\nKey package versions:\")\nfor pkg in [\"tiatoolbox\", \"idc-index\", \"numpy\", \"matplotlib\",\n            \"openslide-bin\", \"torch\", \"highdicom\", \"wsidicom\", \"shapely\"]:\n    try:\n        print(f\"  {pkg}: {version(pkg)}\")\n    except PackageNotFoundError:\n        pass\n\ntry:\n    import psutil\n    ram = psutil.virtual_memory()\n    print(f\"\\nRAM: {ram.total / (1024**3):.1f} GB total, {ram.available / (1024**3):.1f} GB available\")\nexcept ImportError:\n    pass\n\ntry:\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n        capture_output=True, text=True, timeout=5,\n    )\n    if result.returncode == 0:\n        print(f\"GPU:  {result.stdout.strip()}\")\n    else:\n        print(\"GPU:  Not available\")\nexcept (FileNotFoundError, subprocess.TimeoutExpired):\n    print(\"GPU:  Not available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select and Download a Breast Cancer Slide\n",
    "\n",
    "The BCSS model was trained on breast cancer tissue, so we'll select a slide from `tcga_brca` (TCGA Breast Invasive Carcinoma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "idc_client = IDCClient()\nidc_client.fetch_index(\"sm_index\")\n\ncandidates = idc_client.sql_query(\"\"\"\n    SELECT\n        i.SeriesInstanceUID,\n        i.PatientID,\n        ROUND(i.series_size_MB, 1) as size_mb,\n        s.ObjectiveLensPower,\n        s.max_TotalPixelMatrixColumns as width,\n        s.max_TotalPixelMatrixRows as height,\n        s.min_PixelSpacing_2sf as pixel_spacing_mm\n    FROM sm_index s\n    JOIN index i ON s.SeriesInstanceUID = i.SeriesInstanceUID\n    WHERE i.collection_id = 'tcga_brca'\n        AND s.ObjectiveLensPower >= 20\n    ORDER BY i.series_size_MB ASC\n    LIMIT 5\n\"\"\")\n\nselected = candidates.iloc[0]\nseries_uid = selected['SeriesInstanceUID']\nprint(f\"Selected: {selected['PatientID']}, {selected['size_mb']} MB\")\nprint(f\"  Dimensions: {selected['width']}x{selected['height']} @ {selected['ObjectiveLensPower']}x\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "download_dir = './slides'\nos.makedirs(download_dir, exist_ok=True)\n\nidc_client.download_from_selection(\n    downloadDir=download_dir,\n    seriesInstanceUID=[series_uid],\n    dirTemplate='%SeriesInstanceUID'\n)\n\nslide_path = os.path.join(download_dir, series_uid)\nreader = WSIReader.open(slide_path)\n\n# DICOMWSIReader may not populate objective_power or mpp\ninfo = reader.info\nif info.objective_power is None:\n    info.objective_power = float(selected['ObjectiveLensPower'])\nif info.mpp is None:\n    pixel_spacing_um = float(selected['pixel_spacing_mm']) * 1000\n    info.mpp = np.array([pixel_spacing_um, pixel_spacing_um])\n\nthumbnail = reader.slide_thumbnail(resolution=1.25, units=\"power\")\nprint(f\"Opened: {type(reader).__name__}, dimensions: {info.slide_dimensions}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract a Region of Interest\n",
    "\n",
    "Running semantic segmentation on the entire slide can take a long time. For this tutorial, we'll extract a large tissue tile and run segmentation on that.\n",
    "\n",
    "We find a tissue-rich region and extract a tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find a tissue-rich region\ngray = np.mean(thumbnail, axis=2)\ntissue_mask = gray < 200\ntissue_coords = np.argwhere(tissue_mask)\n\ncenter_y, center_x = tissue_coords.mean(axis=0).astype(int)\n\nslide_w, slide_h = info.slide_dimensions\nbaseline_x = int(center_x * slide_w / thumbnail.shape[1])\nbaseline_y = int(center_y * slide_h / thumbnail.shape[0])\n\n# Extract a large tile (~4096x4096) in baseline coordinates\n# DICOMWSIReader has coordinate issues at non-native resolutions,\n# so we read at native resolution and resize if needed.\ntile_size = 4096\nbaseline_mpp = float(info.mpp[0])\ntarget_mpp = 0.5\n\n# Compute how many baseline pixels correspond to tile_size at target_mpp\nbaseline_extent = int(tile_size * target_mpp / baseline_mpp)\n\nbounds = (\n    max(0, baseline_x - baseline_extent // 2),\n    max(0, baseline_y - baseline_extent // 2),\n    min(slide_w, baseline_x + baseline_extent // 2),\n    min(slide_h, baseline_y + baseline_extent // 2),\n)\n\ntile = reader.read_bounds(\n    bounds=bounds,\n    resolution=info.objective_power,\n    units=\"power\",\n)\n\n# Resize to target tile size if native mpp differs from target\nif tile.shape[0] != tile_size or tile.shape[1] != tile_size:\n    tile = np.array(Image.fromarray(tile).resize((tile_size, tile_size), Image.LANCZOS))\n\nprint(f\"Extracted tile shape: {tile.shape}\")\n\nplt.figure(figsize=(8, 8))\nplt.imshow(tile)\nplt.title(f\"Extracted Tissue Tile ({tile_size}x{tile_size} @ ~{target_mpp} mpp)\", fontsize=14)\nplt.axis('off')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save tile as a temporary image for the segmentor\ntile_path = './tile_for_segmentation.png'\nImage.fromarray(tile).save(tile_path)\nprint(f\"Tile saved to {tile_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Semantic Segmentation\n",
    "\n",
    "The `SemanticSegmentor` processes images using a sliding window approach with configurable input/output resolutions and patch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the segmentor\n",
    "segmentor = SemanticSegmentor(\n",
    "    pretrained_model=\"fcn_resnet50_unet-bcss\",\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "# BCSS class names\n",
    "bcss_classes = ['Tumor', 'Stroma', 'Inflammatory', 'Necrosis', 'Other']\n",
    "print(f\"Model: fcn_resnet50_unet-bcss\")\n",
    "print(f\"Classes: {bcss_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run segmentation on the tile\noutput = segmentor.predict(\n    imgs=[tile_path],\n    mode=\"tile\",\n    save_dir=\"./seg_results/\",\n    resolution=1.0,\n    units=\"baseline\",\n    device=device,\n)\n\nprint(f\"Segmentation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the segmentation result\n",
    "seg_result = np.load(output[0][1] + '.raw.0.npy')\n",
    "print(f\"Segmentation result shape: {seg_result.shape}\")\n",
    "\n",
    "# Get class predictions (argmax over channels)\n",
    "if seg_result.ndim == 3 and seg_result.shape[2] > 1:\n",
    "    seg_map = np.argmax(seg_result, axis=2)\n",
    "else:\n",
    "    seg_map = seg_result.squeeze()\n",
    "\n",
    "print(f\"Segmentation map shape: {seg_map.shape}\")\n",
    "print(f\"Unique classes: {np.unique(seg_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class colors\n",
    "bcss_colors = {\n",
    "    0: [0.9, 0.2, 0.2],   # Tumor - red\n",
    "    1: [0.6, 0.6, 0.6],   # Stroma - gray\n",
    "    2: [0.2, 0.6, 0.9],   # Inflammatory - blue\n",
    "    3: [0.1, 0.1, 0.1],   # Necrosis - dark\n",
    "    4: [0.3, 0.8, 0.3],   # Other - green\n",
    "}\n",
    "\n",
    "# Create colored segmentation map\n",
    "seg_colored = np.zeros((*seg_map.shape, 3), dtype=np.float32)\n",
    "for cls_id, color in bcss_colors.items():\n",
    "    seg_colored[seg_map == cls_id] = color\n",
    "\n",
    "# Resize to match tile dimensions if needed\n",
    "from PIL import Image as PILImage\n",
    "if seg_colored.shape[:2] != tile.shape[:2]:\n",
    "    seg_colored_resized = np.array(\n",
    "        PILImage.fromarray((seg_colored * 255).astype(np.uint8)).resize(\n",
    "            (tile.shape[1], tile.shape[0]), PILImage.NEAREST\n",
    "        )\n",
    "    ) / 255.0\n",
    "else:\n",
    "    seg_colored_resized = seg_colored\n",
    "\n",
    "# Create overlay\n",
    "alpha = 0.4\n",
    "overlay = alpha * seg_colored_resized + (1 - alpha) * tile / 255.0\n",
    "overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "axes[0].imshow(tile)\n",
    "axes[0].set_title(\"Original Tile\", fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(seg_colored_resized)\n",
    "axes[1].set_title(\"Segmentation Map\", fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(overlay)\n",
    "axes[2].set_title(\"Segmentation Overlay\", fontsize=14)\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Add legend\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=bcss_colors[i], label=bcss_classes[i])\n",
    "    for i in range(len(bcss_classes))\n",
    "]\n",
    "axes[2].legend(handles=legend_patches, loc='lower right', fontsize=11, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantify Tissue Composition\n",
    "\n",
    "Let's calculate what fraction of the tissue is composed of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area fractions\n",
    "total_pixels = seg_map.size\n",
    "print(\"Tissue composition:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "fractions = {}\n",
    "for cls_id, cls_name in enumerate(bcss_classes):\n",
    "    count = (seg_map == cls_id).sum()\n",
    "    fraction = count / total_pixels * 100\n",
    "    fractions[cls_name] = fraction\n",
    "    print(f\"  {cls_name:15s}: {fraction:6.1f}%\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = [bcss_colors[i] for i in range(len(bcss_classes))]\n",
    "bars = ax.bar(bcss_classes, fractions.values(), color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_ylabel(\"Area Fraction (%)\", fontsize=12)\n",
    "ax.set_title(\"Tissue Composition of Selected Region\", fontsize=14)\n",
    "for bar, val in zip(bars, fractions.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f\"{val:.1f}%\", ha='center', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we learned how to:\n",
    "\n",
    "- Use `SemanticSegmentor` with the pretrained `fcn_resnet50_unet-bcss` model for pixel-level tissue segmentation\n",
    "- Extract a tissue tile from an IDC slide and run segmentation in **tile mode**\n",
    "- Visualize segmentation results as colored maps and overlays\n",
    "- Quantify tissue composition by calculating area fractions\n",
    "\n",
    "**Key difference from Notebook 04:** Patch classification assigns one label per patch, while semantic segmentation assigns a label to every pixel, providing much finer spatial detail.\n",
    "\n",
    "**Next:** [Notebook 06](06_nucleus_instance_segmentation.ipynb) demonstrates individual nucleus detection and classification with HoVer-Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "- **IDC:** Fedorov, A., et al. \"National Cancer Institute Imaging Data Commons: Toward Transparency, Reproducibility, and Scalability in Imaging Artificial Intelligence.\" *RadioGraphics* 43.12 (2023). https://doi.org/10.1148/rg.230180\n",
    "- **TIAToolbox:** Pocock, J., et al. \"TIAToolbox as an end-to-end library for advanced tissue image analytics.\" *Communications Medicine* 2, 120 (2022). https://doi.org/10.1038/s43856-022-00186-5\n",
    "- **BCSS:** Amgad, M., et al. \"Structured crowdsourcing enables convolutional segmentation of histology images.\" *Bioinformatics* 35.18 (2019). https://doi.org/10.1093/bioinformatics/btz083"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}